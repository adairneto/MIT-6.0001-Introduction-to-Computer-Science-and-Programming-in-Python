# Lecture 10: Understanding Program Efficiency, Part 1

Tags: #algorithms #python #efficiency #optimization

## Introduction 

How efficient is my algorithm? How to measure it?

How my design choices relates to the efficiency of my algorithm?

Is there a limit on the amount of time we need to solve a particular problem?

Efficiency may be thought in terms of space or time. We'll focus on time efficiency.

Efficiency can be evaluated by:

- Using a timer
- Counting the operations
- Abstract notion of order of growth

### Time module

```python
import time

def c_to_f(c):
    return c*9/5 + 32

t0 = time.clock()
c_to_f(100000)
t1 = time.clock() - t0
print("t =", t, ":", t1, "s")
```

Problem: varies between implementations and, worse, computers. Also, it is not predictable.

Cannot express a relationship between inputs and time

### Counting operations

Assumes that every operation uses the same time

Then count the number of operations executed as a function of size of input

```python
def mysum(x):
    total = 0
    for i in grange(x+1):
        total += i
    return total
```

`mysum` takes 1+3x operations

Good: depends on the algorithm and not on the copmuter

Problems: still depends on the implementation, not clear about which operations to count

## Orders of growth

Focus on what happens when the size of the problem gets arbitrarily large

Compare the time needed (in this measure) to the size of the input

Express efficiency in terms of size of input

### Example: searching for an element in a list

```python
def search_for_elmt(L, e):
    for i in L:
        if i == e:
            return True
    return False
```

Best case: "e" is the first element of the list

Worst case: "e" is not on the list

We can use the worst case (for analyzing complexity) or the average case

Worst case is a **linear relation**: if I double the length, in the worst case it'll take the double of the time

### Goals

Express the growth of program run time as the input grows

Put an upper bound on growth as tight as possible

Not need to be precise: order of growth, not exact growth

Look at largest factors in run time

"Generally we want tight upper bound on growth, as function of size of input, in worst case"

How to measure the order of the growth? 

## Big-O Notation

Measures an upper bound on the asymptotic growth, i.e. order of growth

Used to describe worst case

Evaluates the algorithm, not the computer

### Example: exact steps vs O()

```python
def fact_inter(n):
    """assumes n an int >= 0"""
    answer = 1
    while n > 1:
        answer *= n
        n -= 1
    return answer
```

Counting steps: 2+5n

Using O(), i.e. worst case asymptotic complexity:

- Ignore additive constants
- Ignore multiplicative constants
- Focus on the term that grows most rapidly

In Big-O, the order of growth is n and we denote O(n)

### Simplification example

- Drop constants and multiplicative factors
- Focus on dominant terms

$n^2 + 2n + 2$ is of the order $O(n^2)$

$n^2 + 100000n + 3^{1000}$ is of the order O(n^2)$

$log(n) + n + 4$ is of the order O(n)$

$0.0001\cdot n \cdot log(n) + 300n$ is of the order $O(n\cdot log(n))$

$2n^{30} + 3^n \text{ is of the order } O(3^n)$

![](/home/adair/Dropbox/Projects/MIT 6.0001 Introduction to Computer Science and Programming in Python/20210127001639 Types of orders.png)

## Complexity classes

### Analyzing programs and their complexity

- **Combine** complexity classes

  - Analyze statements inside functions
  - Apply some rules, focus on dominant term

- **Law of Addition** for O():

  - Used with sequential statements

  - $O(f(n)) + O(g(n))$ is $O(f(n) + g(n))$

  - Example:

    ```python
    for i in range(n):
        print('a') # O(n)
    for j in range(n*n):
        print('b') # O(n*n)
    ```

    is $O(n) + O(n*n) = O (n+n^2) = O(n^2)$ because of the dominant term
  
- **Law of Multiplication** for O():

  - Used with nested statements/loops

  - $O(f(n))\cdot O(g(n))$ is $O(f(n) \cdot g(n))$

  - Example:

    ```python
    for i in range(n):
        for j in range(n): 
            print('a') # O(n) -> n loops of O(n) each give O(n)*O(n)
    ```

    is $O(n) \cdot O(n) = O(n\cdot n) = O(n^2)$

### Hierarchy of complexity classes

![](/home/adair/Dropbox/Projects/MIT 6.0001 Introduction to Computer Science and Programming in Python/20210127001654 Complexity classes.png)

### Identifying common algorithms and their complexity

#### Unsorted list

```python
def linear_search(L, e):
    found = False
    for i in range(len(L)):
        if e == L[i]:
            found = True
    return found
```

Looping n times, so it's a **linear growth** (simple iterative loop are typically linear in complexity)

Notice that if `found = True` we can stop the loop, but it won't change the worst case

Observation: constant time list access

#### Sorted list

```python
def search(L, e):
    for i in range(len(L)):
        if L[i] == e:
            return True
        if L[i] > e:
            return False
    return False
```

$O(len(L))$ for the loop * $O(1)$ to test if `e == L[i]`

Overall complexity: O(n)

Order of growth is same, but the run time may differ

#### Nested loops

```python
def isSubset(L1, L2):
    for e1 in L1:
        matched = False
        for e2 in L2:
            if e1 == e2:
                matched = True
                break
        if not matched:
            return False
    return True
```

Outer loop executed len(L1) times

Each iteration will execute inner loop up to len(L2) times, with constant number of operations

Thus, by multiplication law, the order of growth is $O(len(L1) \cdot len(L2))$ 

In the worst case, then L1 and L2 have the same length, none of elements of L1 in L2, $O(len(L1)^2)$ (quadratic complexity)

#### Intersection

Find the intersection of two lists, return a list with each element appearing only once

```python
def intersect(L1, L2):
    tmp = []
    for e1 in L1:
        for e2 in L2:
            if e1 == e2:
                tmp.append(e1)
    res = []
    for e in tmp:
        if not(e in res):
            res.append(e)
    return res
```

First nested loop takes $len(L1) \cdot len(L2)$ steps

Second loop takes at most $len(L1)$ steps

Determining if element in list might take $len(L1)$ steps

If we assume that the lists are of roughly same length, then $O(len(L1)^2)$

When dealing with **nested loops**. loke at the ranges

Nested loops, each iterating n times, gives $O(n^2)$ i.e. quadratic complexity